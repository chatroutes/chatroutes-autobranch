{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Branch Detection Demo - Interactive Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chatroutes/chatroutes-autobranch/blob/master/notebooks/branch_detection_demo.ipynb)\n",
    "\n",
    "Welcome! This notebook demonstrates the **Branch Detection** module from `chatroutes-autobranch`.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Identify branch points** in text (decision points with multiple options)\n",
    "2. **Count possible paths** (combinatorial analysis)\n",
    "3. **Get statistics** about branching complexity\n",
    "4. **Optional LLM assist** for complex cases\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Analyze LLM responses for decision points\n",
    "- Count possible conversation paths\n",
    "- Estimate branching complexity before generation\n",
    "- Extract structured choices from unstructured text\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## Step 1: Installation\n",
    "\n",
    "Install the library (takes ~30 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": "# Install latest version from GitHub (includes v1.1.0 branch detection)\n!pip install --upgrade -q git+https://github.com/chatroutes/chatroutes-autobranch.git\nprint(\"✅ Installation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## Step 2: Import Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_code"
   },
   "outputs": [],
   "source": [
    "from chatroutes_autobranch import (\n",
    "    BranchExtractor,\n",
    "    BranchPoint,\n",
    "    BranchOption,\n",
    "    LLMBranchParser,\n",
    ")\n",
    "\n",
    "print(\"✅ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example1"
   },
   "source": [
    "## Example 1: Basic Branch Detection\n",
    "\n",
    "Let's analyze a simple text with explicit options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example1_code"
   },
   "outputs": [],
   "source": [
    "# Sample text with branch points\n",
    "text = \"\"\"\n",
    "For your web application, you have several options:\n",
    "\n",
    "Backend Framework:\n",
    "1. Flask - lightweight and flexible\n",
    "2. FastAPI - modern and fast\n",
    "3. Django - batteries included\n",
    "\n",
    "Database:\n",
    "You can use Postgres or MySQL for relational data.\n",
    "\n",
    "Caching:\n",
    "If you need high performance then use Redis else skip caching.\n",
    "\"\"\"\n",
    "\n",
    "# Create extractor and analyze\n",
    "extractor = BranchExtractor()\n",
    "branch_points = extractor.extract(text)\n",
    "\n",
    "print(f\"Found {len(branch_points)} branch points:\\n\")\n",
    "\n",
    "for i, bp in enumerate(branch_points, 1):\n",
    "    print(f\"{i}. {bp.type.upper()}\")\n",
    "    print(f\"   Options: {bp.get_option_labels()}\")\n",
    "    print()\n",
    "\n",
    "# Calculate total combinations\n",
    "max_leaves = extractor.count_max_leaves(branch_points)\n",
    "print(f\"\\n🌳 Maximum possible paths: {max_leaves}\")\n",
    "print(f\"   (3 backends × 2 databases × 2 caching = {3*2*2})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example2"
   },
   "source": [
    "## Example 2: Get Detailed Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example2_code"
   },
   "outputs": [],
   "source": [
    "stats = extractor.get_statistics(branch_points)\n",
    "\n",
    "print(\"📊 BRANCH STATISTICS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total branch points:     {stats['total_branch_points']}\")\n",
    "print(f\"Total options:           {stats['total_options']}\")\n",
    "print(f\"Max possible leaves:     {stats['max_leaves']}\")\n",
    "print(f\"Avg options per branch:  {stats['avg_options_per_branch']:.1f}\")\n",
    "print(f\"\\nBreakdown by type:\")\n",
    "for type_name, count in stats['by_type'].items():\n",
    "    print(f\"  {type_name:15s}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive"
   },
   "source": [
    "## 🎮 Interactive: Try Your Own Text!\n",
    "\n",
    "Edit the text below and run the cell to analyze it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive_code"
   },
   "outputs": [],
   "source": [
    "# ✏️ EDIT THIS TEXT - Try adding your own options!\n",
    "your_text = \"\"\"\n",
    "For your project, consider:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Go\n",
    "\n",
    "You can use Docker or native deployment.\n",
    "\n",
    "If budget allows then use AWS else use Heroku.\n",
    "\"\"\"\n",
    "\n",
    "# Analyze\n",
    "extractor = BranchExtractor()\n",
    "branch_points = extractor.extract(your_text)\n",
    "\n",
    "print(\"YOUR TEXT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nFound {len(branch_points)} branch points:\")\n",
    "\n",
    "if len(branch_points) > 0:\n",
    "    for i, bp in enumerate(branch_points, 1):\n",
    "        print(f\"\\n{i}. {bp.type}:\")\n",
    "        for j, opt in enumerate(bp.options, 1):\n",
    "            print(f\"   {j}) {opt.label}\")\n",
    "    \n",
    "    max_leaves = extractor.count_max_leaves(branch_points)\n",
    "    print(f\"\\n🌳 Total possible paths: {max_leaves}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No branch points detected.\")\n",
    "    print(\"Try adding:\")\n",
    "    print(\"  - Numbered lists (1. 2. 3.)\")\n",
    "    print(\"  - Bullets (- * •)\")\n",
    "    print(\"  - 'or' patterns (A or B)\")\n",
    "    print(\"  - 'if...then...else' patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "patterns"
   },
   "source": [
    "## 📚 Pattern Reference\n",
    "\n",
    "The extractor detects these patterns:\n",
    "\n",
    "### 1. Enumerations\n",
    "```\n",
    "Options:\n",
    "1. Flask\n",
    "2. FastAPI\n",
    "```\n",
    "\n",
    "### 2. Disjunctions\n",
    "```\n",
    "Use Flask or FastAPI\n",
    "```\n",
    "\n",
    "### 3. Conditionals\n",
    "```\n",
    "If you need speed then use FastAPI else use Flask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example3"
   },
   "source": [
    "## Example 3: Analyzing an LLM Response\n",
    "\n",
    "Let's analyze a realistic LLM response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example3_code"
   },
   "outputs": [],
   "source": [
    "llm_response = \"\"\"\n",
    "Based on your requirements, here's my recommendation:\n",
    "\n",
    "**Backend Options:**\n",
    "1. Flask - Best for small to medium projects. Lightweight and flexible.\n",
    "2. FastAPI - Modern choice with excellent async support. Great for APIs.\n",
    "3. Django - Full-featured framework. Choose if you need admin panel and ORM.\n",
    "\n",
    "**Database Selection:**\n",
    "For your use case, I'd suggest either PostgreSQL or MySQL. PostgreSQL has better\n",
    "JSON support, while MySQL has wider adoption.\n",
    "\n",
    "**Deployment:**\n",
    "- Vercel (easiest, zero config)\n",
    "- Fly.io (good balance)\n",
    "- AWS (most control)\n",
    "\n",
    "**Additional Services:**\n",
    "If you need caching then add Redis, otherwise you can skip it for now.\n",
    "\"\"\"\n",
    "\n",
    "extractor = BranchExtractor()\n",
    "branch_points = extractor.extract(llm_response)\n",
    "\n",
    "print(\"LLM RESPONSE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nDetected {len(branch_points)} decision points:\")\n",
    "\n",
    "for i, bp in enumerate(branch_points, 1):\n",
    "    print(f\"\\n{i}. Decision Type: {bp.type.upper()}\")\n",
    "    print(f\"   Choices ({len(bp.options)}):\")\n",
    "    for opt in bp.options:\n",
    "        # Show first 60 chars of each option\n",
    "        label = opt.label[:60] + \"...\" if len(opt.label) > 60 else opt.label\n",
    "        print(f\"     • {label}\")\n",
    "\n",
    "stats = extractor.get_statistics(branch_points)\n",
    "print(f\"\\n📈 COMPLEXITY METRICS\")\n",
    "print(f\"   Total decision points:   {stats['total_branch_points']}\")\n",
    "print(f\"   Total options:           {stats['total_options']}\")\n",
    "print(f\"   Max configurations:      {stats['max_leaves']}\")\n",
    "print(f\"   Branching factor:        {stats['avg_options_per_branch']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llm_section"
   },
   "source": [
    "## 🤖 Optional: Using LLM for Complex Cases\n",
    "\n",
    "For texts with implicit choices, you can use an LLM parser.\n",
    "\n",
    "**Note**: This requires an LLM API (OpenAI, Anthropic, etc.). Skip this section if you don't have one configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llm_setup"
   },
   "outputs": [],
   "source": [
    "# Optional: Set up your LLM (skip if you don't have API access)\n",
    "USE_LLM = False  # Set to True if you want to try LLM parsing\n",
    "\n",
    "if USE_LLM:\n",
    "    # Example with OpenAI (uncomment and configure)\n",
    "    # !pip install -q openai\n",
    "    # import openai\n",
    "    # openai.api_key = \"your-api-key-here\"\n",
    "    \n",
    "    # def my_llm(prompt: str) -> str:\n",
    "    #     response = openai.ChatCompletion.create(\n",
    "    #         model=\"gpt-3.5-turbo\",\n",
    "    #         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    #         temperature=0.1\n",
    "    #     )\n",
    "    #     return response.choices[0].message.content\n",
    "    \n",
    "    # parser = LLMBranchParser(llm=my_llm)\n",
    "    \n",
    "    print(\"✅ LLM parser configured\")\n",
    "else:\n",
    "    print(\"ℹ️  LLM parsing disabled (set USE_LLM = True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## 📊 Comparison: Simple vs Complex Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comparison_code"
   },
   "outputs": [],
   "source": [
    "texts = {\n",
    "    \"Simple (Explicit)\": \"\"\"\n",
    "    Choose:\n",
    "    1. Option A\n",
    "    2. Option B\n",
    "    \n",
    "    Use X or Y.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Medium (Mixed)\": \"\"\"\n",
    "    Backend: Flask or FastAPI or Django\n",
    "    Database: Postgres or MySQL\n",
    "    Cache: Redis or Memcached\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Complex (Nested)\": \"\"\"\n",
    "    Programming Language:\n",
    "    1. Python (easy to learn)\n",
    "    2. JavaScript (web-focused)\n",
    "    3. Go (high performance)\n",
    "    \n",
    "    If Python then use Flask or Django.\n",
    "    If JavaScript then use Express or Nest.\n",
    "    If Go then use Gin or Echo.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "extractor = BranchExtractor()\n",
    "\n",
    "print(\"COMPLEXITY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, text in texts.items():\n",
    "    branch_points = extractor.extract(text)\n",
    "    stats = extractor.get_statistics(branch_points)\n",
    "    \n",
    "    print(f\"\\n{name:20s}: {stats['total_branch_points']} branch points, \"\n",
    "          f\"{stats['max_leaves']} possible paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usecase"
   },
   "source": [
    "## 💡 Real-World Use Case: Conversation Analysis\n",
    "\n",
    "Analyze a multi-turn conversation to find branching opportunities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usecase_code"
   },
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"text\": \"Help me build a web app\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"text\": \"\"\"\n",
    "        Sure! Let's start with the tech stack:\n",
    "        \n",
    "        Frontend:\n",
    "        1. React (most popular)\n",
    "        2. Vue (easier learning curve)\n",
    "        3. Svelte (modern, fast)\n",
    "        \n",
    "        For styling, use Tailwind or Bootstrap.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"text\": \"I'll go with React. What about backend?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"text\": \"\"\"\n",
    "        Great choice! For backend with React:\n",
    "        \n",
    "        - Node.js (same language as frontend)\n",
    "        - Python (more versatile)\n",
    "        - Go (best performance)\n",
    "        \n",
    "        If you choose Node.js then use Express or Nest.\n",
    "        If Python then use Flask or FastAPI.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "extractor = BranchExtractor()\n",
    "\n",
    "print(\"CONVERSATION BRANCH ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_branches = 0\n",
    "total_paths = 1\n",
    "\n",
    "for i, turn in enumerate(conversation):\n",
    "    if turn[\"role\"] == \"assistant\":\n",
    "        branch_points = extractor.extract(turn[\"text\"])\n",
    "        \n",
    "        if len(branch_points) > 0:\n",
    "            print(f\"\\nAssistant Turn {i//2 + 1}:\")\n",
    "            print(f\"  Branch points: {len(branch_points)}\")\n",
    "            \n",
    "            for bp in branch_points:\n",
    "                print(f\"    • {bp.type}: {len(bp.options)} options\")\n",
    "            \n",
    "            max_leaves = extractor.count_max_leaves(branch_points)\n",
    "            print(f\"  Paths at this turn: {max_leaves}\")\n",
    "            \n",
    "            total_branches += len(branch_points)\n",
    "            total_paths *= max_leaves\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total branch points in conversation: {total_branches}\")\n",
    "print(f\"Total possible conversation paths:   {total_paths}\")\n",
    "print(f\"\\n💡 Insight: This conversation could branch into {total_paths} different paths!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 🎯 Summary\n",
    "\n",
    "You've learned how to:\n",
    "\n",
    "✅ **Extract branch points** from text using deterministic patterns  \n",
    "✅ **Count possible paths** (combinatorial analysis)  \n",
    "✅ **Get statistics** about branching complexity  \n",
    "✅ **Analyze conversations** for decision points  \n",
    "✅ **Optional LLM assist** for complex cases  \n",
    "\n",
    "## 📖 Next Steps\n",
    "\n",
    "1. **Combine with branch selection**: Use detected branch points to generate candidates, then filter with the main pipeline\n",
    "2. **Explore other features**: Check out beam search, novelty filtering, and entropy stopping\n",
    "3. **Read the docs**: [Full Documentation](https://github.com/chatroutes/chatroutes-autobranch)\n",
    "\n",
    "## 🔗 Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/chatroutes/chatroutes-autobranch)\n",
    "- [PyPI Package](https://pypi.org/project/chatroutes-autobranch/)\n",
    "- [Full Specification](https://github.com/chatroutes/chatroutes-autobranch/blob/master/chatroutes_autobranch_v1.0.md)\n",
    "- [More Examples](https://github.com/chatroutes/chatroutes-autobranch/tree/master/examples)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy branching!** 🌳"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Branch Detection Demo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}