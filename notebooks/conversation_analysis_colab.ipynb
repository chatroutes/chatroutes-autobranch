{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Analysis & Branch Detection - Interactive Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chatroutes/chatroutes-autobranch/blob/master/notebooks/conversation_analysis_colab.ipynb)\n",
    "\n",
    "Welcome! This notebook demonstrates **advanced conversation analysis** with intelligent branch detection.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Hybrid Detection** - Explicit + Semantic branch detection (no LLM needed!)\n",
    "2. **Conversation Analysis** - Track topic shifts, decision points, Q‚ÜíA transitions\n",
    "3. **Flexible LLM Integration** - Use ANY LLM (OpenAI, Anthropic, Ollama, Groq, etc.)\n",
    "4. **Intelligent Routing** - Automatic decision: when to use LLM vs hybrid\n",
    "5. **Real-World Examples** - Article planning, conversation mapping\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- ‚ö° **Fast by default** - Hybrid detection is instant and free\n",
    "- üéØ **Opt-in LLM** - Only use LLM when you explicitly enable it\n",
    "- üß† **Smart routing** - Heuristics decide when LLM adds value\n",
    "- üîå **Pluggable** - Works with any LLM provider\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installation\n",
    "\n",
    "Install the latest version (takes ~30 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --upgrade -q chatroutes-autobranch\n",
    "\n",
    "# Show installed version\n",
    "import chatroutes_autobranch\n",
    "print(f\"‚úÖ Installation complete! Version: {chatroutes_autobranch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatroutes_autobranch.branch_detection import (\n",
    "    BranchExtractor,\n",
    "    ConversationFlowAnalyzer,\n",
    "    ConversationTurn,\n",
    "    LLMBranchParser,\n",
    "    AutoRouter,\n",
    "    IntelligentRouter,\n",
    ")\n",
    "from chatroutes_autobranch.core.embeddings import DummyEmbeddingProvider\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(\"\\nüì¶ Available Components:\")\n",
    "print(\"  ‚Ä¢ BranchExtractor - Detects explicit patterns\")\n",
    "print(\"  ‚Ä¢ ConversationFlowAnalyzer - Hybrid detection (explicit + semantic)\")\n",
    "print(\"  ‚Ä¢ LLMBranchParser - Optional LLM enhancement\")\n",
    "print(\"  ‚Ä¢ AutoRouter - Intelligent LLM routing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Pattern Detection (Instant, No LLM)\n",
    "\n",
    "Start with the fastest method - pure pattern matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample conversation with clear decisions\n",
    "text = \"\"\"\n",
    "For your project, consider:\n",
    "\n",
    "BRANCH: Tech Stack Decision\n",
    "OPTIONS:\n",
    "1. Python + Flask - Simple, great for beginners\n",
    "2. Node.js + Express - JavaScript everywhere\n",
    "3. Go + Gin - High performance\n",
    "\n",
    "Database: Use PostgreSQL or MySQL.\n",
    "\n",
    "If you need caching then add Redis else skip it.\n",
    "\"\"\"\n",
    "\n",
    "# Extract patterns (instant!)\n",
    "extractor = BranchExtractor()\n",
    "branches = extractor.extract(text)\n",
    "\n",
    "print(f\"‚ö° INSTANT DETECTION (No LLM)\")\n",
    "print(f\"Found {len(branches)} branch points:\\n\")\n",
    "\n",
    "for i, branch in enumerate(branches, 1):\n",
    "    print(f\"{i}. {branch.type.upper()}\")\n",
    "    print(f\"   Options: {branch.get_option_labels()}\")\n",
    "    print()\n",
    "\n",
    "stats = extractor.get_statistics(branches)\n",
    "print(f\"\\nüìä Total combinations: {stats['max_leaves']}\")\n",
    "print(f\"üí° Insight: 3 stacks √ó 2 databases √ó 2 cache options = {3*2*2} paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Conversation Flow Analysis (Hybrid)\n",
    "\n",
    "Analyze a natural conversation - finds topic shifts, decision points, Q‚ÜíA transitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real conversation example\n",
    "conversation = [\n",
    "    ConversationTurn(\n",
    "        id=\"1\",\n",
    "        speaker=\"user\",\n",
    "        content=\"I'm wondering whether to focus on philosophy or practical advice for my article.\"\n",
    "    ),\n",
    "    ConversationTurn(\n",
    "        id=\"2\",\n",
    "        speaker=\"user\",\n",
    "        content=\"Actually, I want to do both. Maybe a multi-article series.\"\n",
    "    ),\n",
    "    ConversationTurn(\n",
    "        id=\"3\",\n",
    "        speaker=\"user\",\n",
    "        content=\"\"\"BRANCH: Article Structure\n",
    "OPTIONS:\n",
    "1. Philosophical - Deep dive into meaning and value\n",
    "2. Practical - Actionable advice for professionals  \n",
    "3. Historical - Pattern analysis from past transitions\n",
    "4. Comprehensive - Multi-article covering all angles\"\"\"\n",
    "    ),\n",
    "    ConversationTurn(\n",
    "        id=\"4\",\n",
    "        speaker=\"user\",\n",
    "        content=\"I'll go comprehensive. Let me plan the structure now.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Set up hybrid analyzer (explicit + semantic)\n",
    "analyzer = ConversationFlowAnalyzer(\n",
    "    embedding_provider=DummyEmbeddingProvider(dimension=384, seed=42),\n",
    "    topic_shift_threshold=0.6,\n",
    "    enable_explicit=True,\n",
    "    enable_semantic=True,\n",
    ")\n",
    "\n",
    "# Analyze (still instant!)\n",
    "results = analyzer.analyze(conversation)\n",
    "\n",
    "print(\"üîç HYBRID ANALYSIS RESULTS\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Explicit branches\n",
    "print(f\"\\n‚úì Explicit Branches: {len(results['explicit_branches'])}\")\n",
    "for branch in results['explicit_branches']:\n",
    "    print(f\"  ‚Ä¢ Turn {branch.meta.get('turn_id')}: {branch.option_count} options\")\n",
    "    for opt in branch.options:\n",
    "        print(f\"    - {opt.label[:60]}\")\n",
    "\n",
    "# Semantic branches\n",
    "print(f\"\\n‚úì Semantic Branches: {len(results['semantic_branches'])}\")\n",
    "for branch in results['semantic_branches']:\n",
    "    print(f\"  ‚Ä¢ Turn {branch.turn_id}: {branch.branch_type}\")\n",
    "    print(f\"    {branch.description}\")\n",
    "    print(f\"    Confidence: {branch.confidence:.2f}\")\n",
    "\n",
    "total = len(results['explicit_branches']) + len(results['semantic_branches'])\n",
    "print(f\"\\nüìä Total: {total} branches detected\")\n",
    "print(\"\\nüí° Key insight: Hybrid found both explicit markers AND implicit patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Flexible LLM Integration\n",
    "\n",
    "Choose YOUR LLM provider! Works with OpenAI, Anthropic, Ollama, Groq, or any custom LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: OpenAI (GPT-3.5, GPT-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenAI (uncomment if you want to use)\n",
    "# !pip install -q openai\n",
    "\n",
    "def setup_openai_llm(api_key: str):\n",
    "    \"\"\"Set up OpenAI as LLM provider.\"\"\"\n",
    "    import openai\n",
    "    \n",
    "    def llm(prompt: str) -> str:\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or \"gpt-4\"\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Example usage (replace with your key)\n",
    "# my_llm = setup_openai_llm(\"your-api-key-here\")\n",
    "# parser = LLMBranchParser(llm=my_llm)\n",
    "# branches = parser.parse(conversation_text)\n",
    "\n",
    "print(\"‚úÖ OpenAI setup function defined\")\n",
    "print(\"   Usage: my_llm = setup_openai_llm('your-api-key')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Anthropic (Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Anthropic (uncomment if you want to use)\n",
    "# !pip install -q anthropic\n",
    "\n",
    "def setup_anthropic_llm(api_key: str):\n",
    "    \"\"\"Set up Anthropic Claude as LLM provider.\"\"\"\n",
    "    import anthropic\n",
    "    \n",
    "    def llm(prompt: str) -> str:\n",
    "        client = anthropic.Anthropic(api_key=api_key)\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",  # or claude-3-sonnet, opus\n",
    "            max_tokens=2000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Example usage\n",
    "# my_llm = setup_anthropic_llm(\"your-api-key-here\")\n",
    "# parser = LLMBranchParser(llm=my_llm)\n",
    "\n",
    "print(\"‚úÖ Anthropic setup function defined\")\n",
    "print(\"   Usage: my_llm = setup_anthropic_llm('your-api-key')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: Groq (Fastest Cloud LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Groq (uncomment if you want to use)\n",
    "# !pip install -q groq\n",
    "\n",
    "def setup_groq_llm(api_key: str):\n",
    "    \"\"\"Set up Groq (fast Llama 3) as LLM provider.\"\"\"\n",
    "    from groq import Groq\n",
    "    \n",
    "    def llm(prompt: str) -> str:\n",
    "        client = Groq(api_key=api_key)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",  # or llama3-8b-8192\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Example usage\n",
    "# my_llm = setup_groq_llm(\"your-api-key-here\")\n",
    "# parser = LLMBranchParser(llm=my_llm)\n",
    "\n",
    "print(\"‚úÖ Groq setup function defined\")\n",
    "print(\"   Usage: my_llm = setup_groq_llm('your-api-key')\")\n",
    "print(\"   Note: Groq is VERY fast (~1s) and cheap!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option D: Ollama (Local, Free)\n",
    "\n",
    "If running locally (not in Colab), you can use Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only works if Ollama is installed locally\n",
    "# !pip install -q ollama\n",
    "\n",
    "def setup_ollama_llm():\n",
    "    \"\"\"Set up Ollama (local Llama 3) as LLM provider.\"\"\"\n",
    "    import ollama\n",
    "    \n",
    "    def llm(prompt: str) -> str:\n",
    "        response = ollama.chat(\n",
    "            model='llama3',\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options={'temperature': 0.1}\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Example usage (only works locally)\n",
    "# my_llm = setup_ollama_llm()\n",
    "# parser = LLMBranchParser(llm=my_llm)\n",
    "\n",
    "print(\"‚úÖ Ollama setup function defined\")\n",
    "print(\"   Note: Only works if Ollama installed locally\")\n",
    "print(\"   Install: https://ollama.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option E: Custom LLM (Bring Your Own!)\n",
    "\n",
    "You can use ANY LLM - just provide a function that takes a prompt and returns text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Your custom LLM function.\n",
    "    \n",
    "    Requirements:\n",
    "    - Takes a string prompt as input\n",
    "    - Returns a string response\n",
    "    - That's it!\n",
    "    \"\"\"\n",
    "    # Call your LLM API here\n",
    "    # response = your_llm_api.call(prompt)\n",
    "    # return response\n",
    "    \n",
    "    # Placeholder for demo\n",
    "    return '{\"branch_points\": []}'\n",
    "\n",
    "# Use it\n",
    "parser = LLMBranchParser(llm=my_custom_llm)\n",
    "\n",
    "print(\"‚úÖ Custom LLM setup\")\n",
    "print(\"   The parser works with ANY function that:\")\n",
    "print(\"     1. Takes: string prompt\")\n",
    "print(\"     2. Returns: string response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Intelligent LLM Routing (Auto-Decide)\n",
    "\n",
    "Let the system decide when to use LLM vs hybrid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversations with different characteristics\n",
    "test_conversations = {\n",
    "    \"Clear & Explicit\": [\n",
    "        ConversationTurn(\n",
    "            id=\"1\",\n",
    "            speaker=\"user\",\n",
    "            content=\"\"\"BRANCH: Choose framework\n",
    "1. Flask\n",
    "2. FastAPI\"\"\"\n",
    "        ),\n",
    "    ],\n",
    "    \n",
    "    \"Ambiguous & Implicit\": [\n",
    "        ConversationTurn(\n",
    "            id=\"1\",\n",
    "            speaker=\"user\",\n",
    "            content=\"I'm torn between philosophy and practical advice. On one hand, deep meaning matters. On the other hand, readers need actionable steps. Perhaps it depends on the audience...\"\n",
    "        ),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Set up analyzer and router\n",
    "analyzer = ConversationFlowAnalyzer(\n",
    "    embedding_provider=DummyEmbeddingProvider(dimension=384, seed=42),\n",
    "    enable_explicit=True,\n",
    "    enable_semantic=True,\n",
    ")\n",
    "\n",
    "# Note: LLM is DISABLED by default\n",
    "router = AutoRouter(\n",
    "    analyzer=analyzer,\n",
    "    # llm_parser=parser,  # Would enable if we had LLM configured\n",
    "    enable_llm_routing=False,  # DEFAULT: LLM disabled\n",
    ")\n",
    "\n",
    "print(\"üß† INTELLIGENT ROUTING DEMO\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, conv in test_conversations.items():\n",
    "    print(f\"\\nüìù Testing: {name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = router.analyze(conv, verbose=True)\n",
    "    \n",
    "    total = len(results['all_branches'])\n",
    "    print(f\"\\n   Result: {total} branches detected\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Default: LLM is DISABLED (fast, free)\")\n",
    "print(\"   ‚Ä¢ Opt-in: Set enable_llm_routing=True\")\n",
    "print(\"   ‚Ä¢ Smart: Router decides when LLM adds value\")\n",
    "print(\"   ‚Ä¢ Hybrid catches most patterns anyway!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Interactive: Analyze Your Own Conversation!\n",
    "\n",
    "Edit the conversation below and see what branches are detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è EDIT THIS - Add your own conversation!\n",
    "your_conversation = [\n",
    "    ConversationTurn(\n",
    "        id=\"1\",\n",
    "        speaker=\"user\",\n",
    "        content=\"I'm planning a new project. Should I use Python or JavaScript?\"\n",
    "    ),\n",
    "    ConversationTurn(\n",
    "        id=\"2\",\n",
    "        speaker=\"user\",\n",
    "        content=\"Actually, maybe I should consider Go for performance. Or Rust?\"\n",
    "    ),\n",
    "    ConversationTurn(\n",
    "        id=\"3\",\n",
    "        speaker=\"user\",\n",
    "        content=\"\"\"Let me be explicit:\n",
    "        \n",
    "BRANCH: Language Choice\n",
    "OPTIONS:\n",
    "1. Python - Easy to learn, great libraries\n",
    "2. JavaScript - Web-focused, huge ecosystem\n",
    "3. Go - Fast, great for services\n",
    "4. Rust - Maximum performance and safety\"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Analyze\n",
    "analyzer = ConversationFlowAnalyzer(\n",
    "    embedding_provider=DummyEmbeddingProvider(dimension=384, seed=42),\n",
    "    enable_explicit=True,\n",
    "    enable_semantic=True,\n",
    ")\n",
    "\n",
    "results = analyzer.analyze(your_conversation)\n",
    "\n",
    "print(\"YOUR CONVERSATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nExplicit branches: {len(results['explicit_branches'])}\")\n",
    "print(f\"Semantic branches: {len(results['semantic_branches'])}\")\n",
    "\n",
    "for i, branch in enumerate(results['explicit_branches'], 1):\n",
    "    print(f\"\\n{i}. Explicit: {branch.option_count} options\")\n",
    "    for opt in branch.options:\n",
    "        print(f\"   ‚Ä¢ {opt.label[:60]}\")\n",
    "\n",
    "for i, branch in enumerate(results['semantic_branches'], 1):\n",
    "    print(f\"\\n{i}. Semantic: {branch.branch_type}\")\n",
    "    print(f\"   {branch.description}\")\n",
    "\n",
    "print(f\"\\nüìä Total: {len(results['explicit_branches']) + len(results['semantic_branches'])} branches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Real-World Example: Article Planning\n",
    "\n",
    "Analyze a conversation about planning an article series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_conversation = [\n",
    "    ConversationTurn(\n",
    "        id=\"1\",\n",
    "        speaker=\"user\",\n",
    "        content=\"I want to write about AI impact on human work.\"\n",
    "    ),\n",
    "    ConversationTurn(\n",
    "        id=\"2\",\n",
    "        speaker=\"user\",\n",
    "        content=\"Should I focus on philosophy or practical advice?\"\n",
    "    ),\n",
    "    ConversationTurn(\n",
    "        id=\"3\",\n",
    "        speaker=\"user\",\n",
    "        content=\"\"\"I think I'll do a multi-article series:\n",
    "        \n",
    "BRANCH: Series Structure\n",
    "OPTIONS:\n",
    "1. Article 1: Philosophy (creativity, meaning, value)\n",
    "2. Article 2: Professions (accountants, lawyers, doctors)\n",
    "3. Article 3: Companies (traditional vs AI-native)\n",
    "4. Article 4: Education (what to study, skills)\"\"\"\n",
    "    ),\n",
    "    ConversationTurn(\n",
    "        id=\"4\",\n",
    "        speaker=\"user\",\n",
    "        content=\"I'll start with Article 1. Need historical references and credible sources.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "analyzer = ConversationFlowAnalyzer(\n",
    "    embedding_provider=DummyEmbeddingProvider(dimension=384, seed=42),\n",
    "    enable_explicit=True,\n",
    "    enable_semantic=True,\n",
    ")\n",
    "\n",
    "results = analyzer.analyze(article_conversation)\n",
    "\n",
    "print(\"üìö ARTICLE PLANNING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüîç Conversation Flow:\")\n",
    "print(\"  Turn 1: Initial idea (AI & work)\")\n",
    "print(\"  Turn 2: Question (philosophy vs practical)\")\n",
    "print(\"  Turn 3: Decision (multi-article series) ‚Üê BRANCH\")\n",
    "print(\"  Turn 4: Action (start with Article 1) ‚Üê COMMITMENT\")\n",
    "\n",
    "print(f\"\\nüìä Branches Detected:\")\n",
    "print(f\"  Explicit: {len(results['explicit_branches'])}\")\n",
    "print(f\"  Semantic: {len(results['semantic_branches'])}\")\n",
    "\n",
    "if results['explicit_branches']:\n",
    "    branch = results['explicit_branches'][0]\n",
    "    print(f\"\\nüìù Article Series Structure ({branch.option_count} articles):\")\n",
    "    for opt in branch.options:\n",
    "        print(f\"  ‚Ä¢ {opt.label}\")\n",
    "\n",
    "for branch in results['semantic_branches']:\n",
    "    print(f\"\\nüß† Semantic: {branch.branch_type}\")\n",
    "    print(f\"   Turn {branch.turn_id}: {branch.description}\")\n",
    "\n",
    "print(\"\\nüí° Insight: The conversation evolved from question ‚Üí decision ‚Üí action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Summary & Best Practices\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "‚úÖ **Pattern Detection** - Instant, deterministic branch extraction  \n",
    "‚úÖ **Hybrid Analysis** - Explicit + Semantic without LLM  \n",
    "‚úÖ **Flexible LLM Integration** - Works with ANY provider  \n",
    "‚úÖ **Intelligent Routing** - Auto-decides when to use LLM  \n",
    "‚úÖ **Real-World Applications** - Article planning, conversation analysis  \n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Method | Speed | Cost | Detection Quality |\n",
    "|--------|-------|------|------------------|\n",
    "| Pattern Only | <0.1s | $0 | Good for explicit |\n",
    "| Hybrid (default) | <0.1s | $0 | Excellent for most cases |\n",
    "| + LLM (opt-in) | 1-5s | ~$0.0001 | Best for complex/ambiguous |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with Hybrid** - It's instant, free, and catches most patterns\n",
    "2. **Add explicit markers** - Use `BRANCH:` and `OPTIONS:` for critical decisions\n",
    "3. **Opt-in to LLM** - Only enable when you need deeper analysis\n",
    "4. **Choose your LLM** - Works with OpenAI, Anthropic, Ollama, Groq, or custom\n",
    "5. **Trust the heuristics** - Intelligent routing makes good decisions\n",
    "\n",
    "### LLM Provider Comparison\n",
    "\n",
    "| Provider | Speed | Cost | Setup |\n",
    "|----------|-------|------|-------|\n",
    "| **Hybrid (No LLM)** | ‚ö° Instant | üí∞ Free | ‚úÖ Works now |\n",
    "| **Groq** | ‚ö° Fast (~1s) | üí∞ Cheap | üîë API key |\n",
    "| **OpenAI** | üêå 2-3s | üí∞üí∞ Moderate | üîë API key |\n",
    "| **Anthropic** | üêå 2-3s | üí∞üí∞ Moderate | üîë API key |\n",
    "| **Ollama** | ‚ö° 2-5s | üí∞ Free | üíª Local only |\n",
    "\n",
    "**Recommendation:** Start with hybrid, add Groq if you need LLM (fastest & cheapest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Next Steps\n",
    "\n",
    "### Explore More Features\n",
    "\n",
    "This notebook focused on branch detection. The library also includes:\n",
    "\n",
    "- **Beam Search** - Select top-K candidates\n",
    "- **Novelty Filtering** - Remove similar options\n",
    "- **Entropy Stopping** - Know when to stop branching\n",
    "- **Budget Management** - Control costs and tokens\n",
    "\n",
    "### Resources\n",
    "\n",
    "- üìñ [GitHub Repository](https://github.com/chatroutes/chatroutes-autobranch)\n",
    "- üì¶ [PyPI Package](https://pypi.org/project/chatroutes-autobranch/)\n",
    "- üìö [Full Documentation](https://github.com/chatroutes/chatroutes-autobranch/blob/master/README.md)\n",
    "- üí° [More Examples](https://github.com/chatroutes/chatroutes-autobranch/tree/master/examples)\n",
    "\n",
    "### Try It Locally\n",
    "\n",
    "```bash\n",
    "pip install chatroutes-autobranch\n",
    "\n",
    "# Run examples\n",
    "python examples/analyze_conversation_hybrid.py\n",
    "python examples/intelligent_routing_demo.py\n",
    "python examples/llama3_branch_detection.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Happy analyzing!** üéØüöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
