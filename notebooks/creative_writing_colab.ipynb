{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ChatRoutes AutoBranch - Creative Writing Demo\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chatroutes/chatroutes-autobranch/blob/master/notebooks/creative_writing_colab.ipynb)\n\nThis notebook demonstrates **chatroutes-autobranch** for creative writing scenarios using:\n- ‚úÖ **100% FREE** local LLMs via Ollama\n- ‚úÖ **FREE** embeddings via sentence-transformers\n- ‚ö° **Optional GPU acceleration** (see cost warning below)\n\n---\n\n## ‚ö†Ô∏è GPU vs CPU: Cost & Performance\n\n| Runtime | Speed (per response) | Cost | Best For |\n|---------|---------------------|------|----------|\n| **CPU (Free)** | ~40-50s | $0 | Testing, learning |\n| **GPU (Free Tier)** | ~1-3s | $0 (limited hours/day) | Quick demos |\n| **GPU (Colab Pro)** | ~1-3s | $10/month | Regular use |\n| **GPU (Colab Pro+)** | ~0.5-1s | $50/month | Heavy use |\n\n**üí° Recommendation**: Start with **CPU (free, no limits)** for learning. Upgrade to GPU if you need speed.\n\n---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Cell 1: Environment Setup & GPU Detection"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Detect GPU\n",
        "def check_gpu():\n",
        "    \"\"\"Check if GPU is available and show details.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
        "                                capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0 and result.stdout.strip():\n",
        "            gpu_info = result.stdout.strip()\n",
        "            print(\"‚úÖ GPU DETECTED:\")\n",
        "            print(f\"   {gpu_info}\")\n",
        "            print(\"\\n‚ö° GPU will significantly speed up inference:\")\n",
        "            print(\"   - Ollama models: 20-40x faster\")\n",
        "            print(\"   - Embeddings: 5-10x faster\")\n",
        "            print(\"   - Total time: ~5-10 minutes (vs 30-40 minutes on CPU)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è  No GPU detected - using CPU\")\n",
        "            print(\"\\nüìä CPU Performance (expected):\")\n",
        "            print(\"   - Ollama llama3.1:8b: ~40-50s per response\")\n",
        "            print(\"   - Total time: ~30-40 minutes for all scenarios\")\n",
        "            print(\"\\nüí° To enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "            return False\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ÑπÔ∏è  No GPU detected - using CPU\")\n",
        "        print(\"\\nüìä CPU Performance: ~30-40 minutes total\")\n",
        "        return False\n",
        "\n",
        "has_gpu = check_gpu()\n",
        "\n",
        "# Show cost warning for GPU\n",
        "if has_gpu:\n",
        "    print(\"\\n‚ö†Ô∏è  GPU COST WARNING:\")\n",
        "    print(\"   - Free tier: Limited GPU hours per day\")\n",
        "    print(\"   - Colab Pro: $10/month for more GPU hours\")\n",
        "    print(\"   - This notebook will use ~10-15 minutes of GPU time\")\n",
        "    print(\"\\n   To switch to CPU (free, unlimited):\")\n",
        "    print(\"   Runtime ‚Üí Change runtime type ‚Üí None\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ CPU is completely FREE with no usage limits!\")\n",
        "    print(\"   Just slower - perfect for learning and testing.\")"
      ],
      "metadata": {
        "id": "gpu_detection"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Cell 2: Install Dependencies\n",
        "\n",
        "This cell installs:\n",
        "1. **chatroutes-autobranch** - The main library\n",
        "2. **sentence-transformers** - Free embeddings\n",
        "3. **Ollama** - Free local LLM server\n",
        "\n",
        "‚è±Ô∏è **First run**: ~2-3 minutes (downloads packages)"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": "# Install chatroutes-autobranch and dependencies\nprint(\"üì¶ Installing chatroutes-autobranch...\")\n!pip install -q --upgrade chatroutes-autobranch sentence-transformers requests\nprint(\"‚úÖ Python packages installed!\")\n\n# Install Ollama\nprint(\"\\nü¶ô Installing Ollama...\")\n!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1\nprint(\"‚úÖ Ollama installed!\")",
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Cell 3: Start Ollama Server\n",
        "\n",
        "Ollama runs as a background server. This cell:\n",
        "1. Starts the Ollama server\n",
        "2. Waits for it to be ready\n",
        "3. Verifies the connection\n",
        "\n",
        "‚ö†Ô∏è **Note**: The server runs until the Colab session ends."
      ],
      "metadata": {
        "id": "start_server_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "\n",
        "# Start Ollama in background\n",
        "ollama_process = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for server to start\n",
        "max_retries = 30\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        response = requests.get('http://localhost:11434/api/tags', timeout=2)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Ollama server is ready!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "    if (i + 1) % 5 == 0:\n",
        "        print(f\"   Waiting for server... ({i+1}/{max_retries}s)\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to start Ollama server\")\n",
        "    raise Exception(\"Ollama server failed to start\")"
      ],
      "metadata": {
        "id": "start_ollama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Cell 4: Download LLM Model\n",
        "\n",
        "Choose a model based on your needs:\n",
        "\n",
        "| Model | Size | Speed (CPU) | Speed (GPU) | Quality | RAM |\n",
        "|-------|------|-------------|-------------|---------|-----|\n",
        "| **llama3.1:8b** | 4.9 GB | ~40s | ~1s | Good | 8GB |\n",
        "| **qwen3:14b** | 9.3 GB | ~80s | ~3s | Excellent | 16GB |\n",
        "| **gpt-oss:20b** | 13 GB | ~120s | ~5s | Best | 24GB |\n",
        "\n",
        "**üí° Recommendation**: Use **llama3.1:8b** for speed, **qwen3:14b** for quality.\n",
        "\n",
        "‚è±Ô∏è **Download time**: 2-5 minutes (one-time per session)"
      ],
      "metadata": {
        "id": "download_model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# CONFIGURE YOUR MODEL HERE\n",
        "MODEL = \"llama3.1:8b\"  # Options: \"llama3.1:8b\", \"qwen3:14b\", \"gpt-oss:20b\"\n",
        "\n",
        "print(f\"üì• Downloading {MODEL}...\")\n",
        "print(f\"   This is a one-time download per Colab session.\")\n",
        "print(f\"   ‚è±Ô∏è  Expected time: 2-5 minutes\\n\")\n",
        "\n",
        "result = subprocess.run(\n",
        "    ['ollama', 'pull', MODEL],\n",
        "    capture_output=False,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(f\"\\n‚úÖ {MODEL} downloaded successfully!\")\n",
        "    \n",
        "    # Test generation\n",
        "    print(\"\\nüß™ Testing model with quick generation...\")\n",
        "    test_result = subprocess.run(\n",
        "        ['ollama', 'run', MODEL, 'Say hello in one sentence.'],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=120\n",
        "    )\n",
        "    \n",
        "    if test_result.returncode == 0:\n",
        "        print(\"‚úÖ Model is working!\")\n",
        "        print(f\"   Response: {test_result.stdout[:100]}...\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Model test failed, but continuing...\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Failed to download {MODEL}\")\n",
        "    raise Exception(f\"Model download failed\")"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Cell 5: Download Embedding Models\n",
        "\n",
        "Sentence-transformers will download embedding models on first use:\n",
        "\n",
        "| Model | Size | Dimension | Quality Score |\n",
        "|-------|------|-----------|---------------|\n",
        "| jina-embeddings-v2-base-en | 560 MB | 768D | 60.3 |\n",
        "| all-mpnet-base-v2 | 420 MB | 768D | 57.8 |\n",
        "| bge-large-en-v1.5 | 1.2 GB | 1024D | 59.5 |\n",
        "\n",
        "**These download automatically when needed** - no action required!\n",
        "\n",
        "‚è±Ô∏è **Download time**: ~1-2 minutes per model (one-time)"
      ],
      "metadata": {
        "id": "embeddings_info"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "print(\"üìä Embedding Model Information:\")\n",
        "print(\"\\nModels will download automatically on first use.\")\n",
        "print(\"Each model downloads once per session (~1-2 min each).\\n\")\n",
        "\n",
        "# Show device info\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"üñ•Ô∏è  Embeddings will use: {device.upper()}\")\n",
        "\n",
        "if device == 'cuda':\n",
        "    print(\"   ‚ö° GPU will accelerate embeddings 5-10x!\")\n",
        "else:\n",
        "    print(\"   üìä CPU embeddings are still fast (~1-2s per batch)\")\n",
        "\n",
        "print(\"\\n‚úÖ Ready to generate embeddings!\")"
      ],
      "metadata": {
        "id": "embeddings_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé® Cell 6: Run Creative Writing Demo\n",
        "\n",
        "This cell runs 4 creative writing scenarios demonstrating different features:\n",
        "\n",
        "1. **AI Memory Story** - High diversity across genres\n",
        "2. **Mars Detective Twists** - Clustering similar plot ideas\n",
        "3. **Rom-Com Endings** - Entropy-based stopping\n",
        "4. **Style Variations** - Intent alignment\n",
        "\n",
        "‚è±Ô∏è **Expected runtime**:\n",
        "- **GPU**: ~5-10 minutes\n",
        "- **CPU**: ~30-40 minutes\n",
        "\n",
        "**üí° Tip**: You can continue working in other tabs while this runs!"
      ],
      "metadata": {
        "id": "demo_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the example script\n!wget -q https://raw.githubusercontent.com/chatroutes/chatroutes-autobranch/master/examples/creative_writting_usage.py\n\n# Run the demo\nprint(\"üé® Starting Creative Writing Demo...\\n\")\nprint(\"=\" * 80)\n\n!python creative_writting_usage.py"
      ],
      "metadata": {
        "id": "run_demo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Cell 7: Performance Comparison (Optional)\n",
        "\n",
        "Compare CPU vs GPU performance with a quick benchmark."
      ],
      "metadata": {
        "id": "benchmark_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"‚ö° Performance Benchmark\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test 1: Ollama inference\n",
        "print(\"\\nü¶ô Test 1: Ollama LLM Generation\")\n",
        "print(f\"   Model: {MODEL}\")\n",
        "print(\"   Prompt: 'Write one sentence about AI.'\\n\")\n",
        "\n",
        "start = time.time()\n",
        "response = requests.post(\n",
        "    'http://localhost:11434/api/generate',\n",
        "    json={\n",
        "        'model': MODEL,\n",
        "        'prompt': 'Write one sentence about AI.',\n",
        "        'stream': False\n",
        "    },\n",
        "    timeout=120\n",
        ")\n",
        "ollama_time = time.time() - start\n",
        "\n",
        "print(f\"   ‚è±Ô∏è  Time: {ollama_time:.2f}s\")\n",
        "print(f\"   üìù Response: {response.json().get('response', '')[:100]}...\")\n",
        "\n",
        "# Test 2: Embeddings\n",
        "print(\"\\nüî¢ Test 2: Sentence Embeddings\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"   Device: {device.upper()}\")\n",
        "print(\"   Model: all-mpnet-base-v2\")\n",
        "print(\"   Texts: 10 sentences\\n\")\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
        "texts = [f\"This is test sentence number {i}.\" for i in range(10)]\n",
        "\n",
        "start = time.time()\n",
        "embeddings = model.encode(texts)\n",
        "embed_time = time.time() - start\n",
        "\n",
        "print(f\"   ‚è±Ô∏è  Time: {embed_time:.3f}s\")\n",
        "print(f\"   üìä Generated: {len(embeddings)} embeddings of {len(embeddings[0])}D\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä BENCHMARK SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nLLM Generation ({MODEL}): {ollama_time:.2f}s per response\")\n",
        "print(f\"Embeddings (10 texts):     {embed_time:.3f}s\")\n",
        "print(f\"\\nRuntime: {'GPU ‚ö°' if device == 'cuda' else 'CPU üê¢'}\")\n",
        "\n",
        "if device == 'cpu':\n",
        "    print(\"\\nüí° Switch to GPU for ~20-40x speedup!\")\n",
        "    print(\"   Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Using GPU acceleration!\")"
      ],
      "metadata": {
        "id": "benchmark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßπ Cell 8: Cleanup (Optional)\n",
        "\n",
        "Stop Ollama server and free up memory."
      ],
      "metadata": {
        "id": "cleanup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "\n",
        "print(\"üßπ Cleaning up...\\n\")\n",
        "\n",
        "# Stop Ollama\n",
        "try:\n",
        "    ollama_process.send_signal(signal.SIGTERM)\n",
        "    ollama_process.wait(timeout=5)\n",
        "    print(\"‚úÖ Ollama server stopped\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Could not stop Ollama (may already be stopped)\")\n",
        "\n",
        "print(\"\\n‚úÖ Cleanup complete!\")\n",
        "print(\"\\nüí° To restart, run the cells again from Cell 3.\")"
      ],
      "metadata": {
        "id": "cleanup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n\n## üíæ How Model Downloads Work in Colab\n\n### Ollama Models\n- **Location**: `/usr/share/ollama/.ollama/models/`\n- **Persistence**: Lost when runtime disconnects\n- **Download**: 2-5 minutes per model\n- **Re-download**: Required each new session\n\n### Sentence-Transformers\n- **Location**: `/root/.cache/huggingface/`\n- **Persistence**: Lost when runtime disconnects\n- **Download**: 1-2 minutes per model (auto on first use)\n- **Re-download**: Required each new session\n\n### Tips for Faster Startup\n\n1. **Use smaller models**:\n   ```python\n   MODEL = \"llama3.1:8b\"  # 4.9 GB, fastest\n   ```\n\n2. **Mount Google Drive** (advanced):\n   ```python\n   from google.colab import drive\n   drive.mount('/content/drive')\n   # Cache models to Drive (persists across sessions)\n   ```\n\n3. **Colab Pro**:\n   - Faster downloads\n   - Longer session timeouts\n   - More GPU availability\n\n---\n\n## üìö Additional Resources\n\n- [ChatRoutes AutoBranch GitHub](https://github.com/chatroutes/chatroutes-autobranch)\n- [Ollama Documentation](https://ollama.ai/docs)\n- [Sentence-Transformers Docs](https://www.sbert.net/)\n- [Google Colab FAQ](https://research.google.com/colaboratory/faq.html)\n\n---\n\n## ‚ùì Troubleshooting\n\n### Problem: \"Ollama server not responding\"\n**Solution**: Restart Cell 3 (Start Ollama Server)\n\n### Problem: \"Model download failed\"\n**Solution**: \n```bash\n# Check available space\n!df -h /\n\n# Use smaller model\nMODEL = \"llama3.1:8b\"  # Only 4.9 GB\n```\n\n### Problem: \"Out of memory\"\n**Solution**:\n- Use smaller model (llama3.1:8b)\n- Restart runtime: Runtime ‚Üí Restart runtime\n- For GPU: Use Colab Pro (more VRAM)\n\n### Problem: \"GPU not detected\"\n**Solution**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n\n---\n\n**Made with ‚ù§Ô∏è using ChatRoutes AutoBranch**"
      ],
      "metadata": {
        "id": "footer"
      }
    }
  ]
}