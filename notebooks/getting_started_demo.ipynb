{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# ChatRoutes AutoBranch - Getting Started Demo\n",
        "\n",
        "**Intelligent branch exploration for LLM-powered applications**\n",
        "\n",
        "[![PyPI version](https://badge.fury.io/py/chatroutes-autobranch.svg)](https://badge.fury.io/py/chatroutes-autobranch)\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n",
        "\n",
        "---\n",
        "\n",
        "## What is ChatRoutes AutoBranch?\n",
        "\n",
        "ChatRoutes AutoBranch helps you:\n",
        "- ðŸŽ¯ **Select the best responses** from multiple LLM outputs (beam search)\n",
        "- ðŸŒˆ **Ensure diversity** in responses (avoid repetition)\n",
        "- ðŸ›‘ **Know when to stop** exploring (entropy-based convergence)\n",
        "- ðŸ’° **Control costs** with budget management (tokens, time, nodes)\n",
        "\n",
        "Perfect for:\n",
        "- Tree-of-thought reasoning\n",
        "- Multi-agent systems\n",
        "- Creative writing\n",
        "- Question answering\n",
        "- Any LLM application that explores multiple paths\n",
        "\n",
        "---\n",
        "\n",
        "## This Demo\n",
        "\n",
        "In this notebook, we'll cover:\n",
        "1. Installation\n",
        "2. Basic beam search\n",
        "3. Scoring strategies\n",
        "4. Novelty filtering\n",
        "5. Complete pipeline example\n",
        "\n",
        "**Time**: ~5 minutes  \n",
        "**Level**: Beginner-friendly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "installation"
      },
      "source": [
        "## Step 1: Installation\n",
        "\n",
        "Install ChatRoutes AutoBranch from PyPI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": "!pip install -q --upgrade chatroutes-autobranch\nprint(\"âœ… Installation complete!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## Step 2: Import the Library\n",
        "\n",
        "Let's import the key components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_code"
      },
      "outputs": [],
      "source": [
        "from chatroutes_autobranch import (\n",
        "    # Core components\n",
        "    BranchSelector,\n",
        "    BeamSelector,\n",
        "    Candidate,\n",
        "    ScoredCandidate,\n",
        "    \n",
        "    # Scoring\n",
        "    CompositeScorer,\n",
        "    \n",
        "    # Novelty filtering\n",
        "    CosineNoveltyFilter,\n",
        "    MMRNoveltyFilter,\n",
        "    \n",
        "    # Budget management\n",
        "    Budget,\n",
        "    BudgetManager,\n",
        "    \n",
        "    # Utilities\n",
        "    DummyEmbeddingProvider,\n",
        ")\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n",
        "print(\"\\nKey components:\")\n",
        "print(\"  â€¢ BranchSelector - Main orchestrator\")\n",
        "print(\"  â€¢ BeamSelector - Keeps top-K candidates\")\n",
        "print(\"  â€¢ Candidate - Represents a branch/response\")\n",
        "print(\"  â€¢ CompositeScorer - Combines multiple scoring strategies\")\n",
        "print(\"  â€¢ NoveltyFilters - Remove similar/redundant responses\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example1"
      },
      "source": [
        "## Example 1: Basic Beam Search\n",
        "\n",
        "Let's start with a simple example: selecting the top 3 responses from 5 candidates based on their confidence scores (logprobs).\n",
        "\n",
        "**Scenario**: You asked an LLM to generate story openings, and got 5 responses with confidence scores. You want to keep only the best 3.\n",
        "\n",
        "**Note**: We create a simple scorer that uses 100% confidence (from logprobs in metadata)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example1_code"
      },
      "outputs": [],
      "source": [
        "# Setup: Create a simple scorer (uses logprobs from metadata)\n",
        "embedding_provider = DummyEmbeddingProvider(dimension=64, seed=42)\n",
        "scorer = CompositeScorer(\n",
        "    weights={\"confidence\": 1.0},  # 100% weight on confidence (logprobs)\n",
        "    embedding_provider=embedding_provider\n",
        ")\n",
        "\n",
        "# Step 1: Create a beam selector (keep top 3)\n",
        "beam = BeamSelector(scorer=scorer, k=3)\n",
        "\n",
        "# Step 2: Create the main selector\n",
        "selector = BranchSelector(beam_selector=beam)\n",
        "\n",
        "# Step 3: Define your candidates\n",
        "# In real use, these would come from your LLM with logprobs/scores\n",
        "parent = Candidate(\n",
        "    id=\"prompt\",\n",
        "    text=\"Write a story opening about a detective\",\n",
        "    meta={\"logprobs\": -0.1}\n",
        ")\n",
        "\n",
        "candidates = [\n",
        "    Candidate(id=\"c1\", text=\"Detective Sarah Chen walked into the dimly lit office...\", meta={\"logprobs\": -0.5}),\n",
        "    Candidate(id=\"c2\", text=\"The rain hammered against the window as Detective Miller reviewed the case...\", meta={\"logprobs\": -0.3}),\n",
        "    Candidate(id=\"c3\", text=\"It was another cold morning when Detective Rodriguez found the first clue...\", meta={\"logprobs\": -0.8}),\n",
        "    Candidate(id=\"c4\", text=\"Detective Park had seen many cases, but this one was different...\", meta={\"logprobs\": -0.4}),\n",
        "    Candidate(id=\"c5\", text=\"The phone rang at 3 AM. Detective Thompson knew it was trouble...\", meta={\"logprobs\": -0.2}),\n",
        "]\n",
        "\n",
        "# Step 4: Select the best 3\n",
        "result = selector.step(parent, candidates)\n",
        "\n",
        "# Display results\n",
        "print(\"=\"*60)\n",
        "print(\"BEAM SEARCH RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input: {len(candidates)} candidates\")\n",
        "print(f\"Output: {len(result.kept)} selected (top {beam.k})\")\n",
        "print()\n",
        "\n",
        "print(\"[OK] SELECTED (Top 3 by confidence):\")\n",
        "for i, candidate in enumerate(result.kept, 1):\n",
        "    score = candidate.meta.get(\"logprobs\", \"N/A\")\n",
        "    print(f\"{i}. [{candidate.id}] Score: {score}\")\n",
        "    print(f\"   {candidate.text}\")\n",
        "    print()\n",
        "\n",
        "# Calculate filtered candidates\n",
        "kept_ids = {c.id for c in result.kept}\n",
        "filtered = [c for c in result.scored if c.id not in kept_ids]\n",
        "\n",
        "print(f\"[X] FILTERED OUT ({len(filtered)}):\")\n",
        "for candidate in filtered:\n",
        "    score = candidate.meta.get(\"logprobs\", \"N/A\")\n",
        "    print(f\"   [{candidate.id}] Score: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example2"
      },
      "source": [
        "## Example 2: Multi-Strategy Scoring\n",
        "\n",
        "Instead of just using raw scores, let's combine multiple factors:\n",
        "- **Confidence** (from logprobs)\n",
        "- **Relevance** (semantic similarity to prompt)\n",
        "- **Novelty** (how different from other candidates)\n",
        "\n",
        "This gives us smarter selection!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example2_code"
      },
      "outputs": [],
      "source": [
        "# Setup embedding provider (for semantic similarity)\n",
        "embedding_provider = DummyEmbeddingProvider(dimension=64, seed=42)\n",
        "\n",
        "# Create a composite scorer with weighted strategies\n",
        "scorer = CompositeScorer(\n",
        "    weights={\n",
        "        \"confidence\": 0.4,  # 40% weight on LLM confidence\n",
        "        \"relevance\": 0.4,   # 40% weight on relevance to prompt\n",
        "        \"novelty\": 0.2,     # 20% weight on being different\n",
        "    },\n",
        "    embedding_provider=embedding_provider\n",
        ")\n",
        "\n",
        "# Create beam with scoring\n",
        "beam = BeamSelector(scorer=scorer, k=3)\n",
        "selector = BranchSelector(beam_selector=beam)\n",
        "\n",
        "# Define parent and candidates (same as Example 1)\n",
        "parent = Candidate(\n",
        "    id=\"prompt\",\n",
        "    text=\"Write a story opening about a detective\",\n",
        "    meta={\"logprobs\": -0.1}\n",
        ")\n",
        "\n",
        "candidates = [\n",
        "    Candidate(id=\"c1\", text=\"Detective Sarah Chen walked into the dimly lit office...\", meta={\"logprobs\": -0.5}),\n",
        "    Candidate(id=\"c2\", text=\"The rain hammered against the window as Detective Miller reviewed the case...\", meta={\"logprobs\": -0.3}),\n",
        "    Candidate(id=\"c3\", text=\"It was another cold morning when Detective Rodriguez found the first clue...\", meta={\"logprobs\": -0.8}),\n",
        "    Candidate(id=\"c4\", text=\"Detective Park had seen many cases, but this one was different...\", meta={\"logprobs\": -0.4}),\n",
        "    Candidate(id=\"c5\", text=\"The phone rang at 3 AM. Detective Thompson knew it was trouble...\", meta={\"logprobs\": -0.2}),\n",
        "]\n",
        "\n",
        "# Select with multi-strategy scoring\n",
        "result = selector.step(parent, candidates)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MULTI-STRATEGY SCORING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"Scoring Strategy:\")\n",
        "print(\"  - 40% Confidence (logprobs from LLM)\")\n",
        "print(\"  - 40% Relevance (similarity to prompt)\")\n",
        "print(\"  - 20% Novelty (uniqueness)\")\n",
        "print()\n",
        "\n",
        "print(\"[OK] SELECTED (Top 3 by composite score):\")\n",
        "for i, candidate in enumerate(result.kept, 1):\n",
        "    print(f\"{i}. [{candidate.id}]\")\n",
        "    print(f\"   {candidate.text[:60]}...\")\n",
        "    if hasattr(candidate, \"score\"):\n",
        "        print(f\"   Final Score: {candidate.score:.3f}\")\n",
        "    print()\n",
        "\n",
        "# Calculate filtered candidates\n",
        "kept_ids = {c.id for c in result.kept}\n",
        "filtered = [c for c in result.scored if c.id not in kept_ids]\n",
        "\n",
        "print()\n",
        "print(\"[!] Notice: Results may differ from Example 1!\")\n",
        "print(\"   We are now considering relevance and novelty, not just confidence.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example3"
      },
      "source": [
        "## Example 3: Novelty Filtering (Remove Duplicates)\n",
        "\n",
        "Sometimes LLMs generate similar responses. Let's filter out near-duplicates using:\n",
        "- **Cosine similarity** - Remove responses that are too similar\n",
        "- **MMR (Maximal Marginal Relevance)** - Balance relevance and diversity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example3_code"
      },
      "outputs": [],
      "source": "# Create candidates with some duplicates\ncandidates_with_dupes = [\n    Candidate(id=\"c1\", text=\"The sky is blue and beautiful today\", meta={\"logprobs\": -0.2}),\n    Candidate(id=\"c2\", text=\"The sky is blue and lovely today\", meta={\"logprobs\": -0.3}),  # Very similar to c1\n    Candidate(id=\"c3\", text=\"Quantum computers use superposition\", meta={\"logprobs\": -0.4}),\n    Candidate(id=\"c4\", text=\"The weather is nice with blue skies\", meta={\"logprobs\": -0.25}),  # Similar to c1\n    Candidate(id=\"c5\", text=\"Machine learning enables pattern recognition\", meta={\"logprobs\": -0.35}),\n]\n\nparent = Candidate(id=\"prompt\", text=\"Tell me something interesting\")\n\nprint(\"=\"*60)\nprint(\"NOVELTY FILTERING DEMO\")\nprint(\"=\"*60)\nprint()\nprint(f\"Input: {len(candidates_with_dupes)} candidates (some are similar)\")\nprint()\n\n# Method 1: Cosine Similarity Filter\nprint()\nprint(\"Method 1: COSINE SIMILARITY FILTER\")\nprint(\"Remove candidates with similarity > 80%\")\nprint()\n\ncosine_filter = CosineNoveltyFilter(\n    threshold=0.80,  # Remove if 80%+ similar\n    embedding_provider=embedding_provider\n)\n\n# FIX: novelty_filter goes in BranchSelector, not BeamSelector!\nbeam = BeamSelector(scorer=scorer, k=5)\nselector_cosine = BranchSelector(beam_selector=beam, novelty_filter=cosine_filter)\nresult_cosine = selector_cosine.step(parent, candidates_with_dupes)\n\n# Calculate filtered\nkept_ids_cosine = {c.id for c in result_cosine.kept}\nfiltered_cosine = [c for c in result_cosine.scored if c.id not in kept_ids_cosine]\n\nprint(f\"[OK] Kept: {len(result_cosine.kept)} unique responses\")\nfor c in result_cosine.kept:\n    print(f\"   [{c.id}] {c.text}\")\n\nprint()\nprint(f\"[X] Filtered: {len(filtered_cosine)} duplicates\")\nfor c in filtered_cosine:\n    print(f\"   [{c.id}] {c.text[:50]}... (too similar)\")\n\n# Method 2: MMR (Maximal Marginal Relevance)\nprint()\nprint()\nprint(\"Method 2: MMR (BALANCED APPROACH)\")\nprint(\"Balance between relevance (70%) and diversity (30%)\")\nprint()\n\nmmr_filter = MMRNoveltyFilter(\n    lambda_param=0.7,  # 70% relevance, 30% diversity\n    embedding_provider=embedding_provider\n)\n\n# FIX: novelty_filter goes in BranchSelector, not BeamSelector!\nbeam_mmr = BeamSelector(scorer=scorer, k=5)\nselector_mmr = BranchSelector(beam_selector=beam_mmr, novelty_filter=mmr_filter)\nresult_mmr = selector_mmr.step(parent, candidates_with_dupes)\n\nprint(f\"[OK] Kept: {len(result_mmr.kept)} diverse responses\")\nfor c in result_mmr.kept:\n    print(f\"   [{c.id}] {c.text}\")\n\nprint()\nprint(\"[!] MMR ensures both quality AND diversity!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example4"
      },
      "source": [
        "## Example 4: Complete Pipeline with Budget Control\n",
        "\n",
        "Let's put it all together with:\n",
        "- âœ… Beam search (top K selection)\n",
        "- âœ… Multi-strategy scoring\n",
        "- âœ… Novelty filtering\n",
        "- âœ… Budget management (prevent runaway costs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "example4_code"
      },
      "outputs": [],
      "source": "print(\"=\"*60)\nprint(\"COMPLETE PIPELINE EXAMPLE\")\nprint(\"=\"*60)\n\n# Setup components\nembedding_provider = DummyEmbeddingProvider(dimension=64, seed=42)\n\n# 1. Scorer: Weighted strategies\nscorer = CompositeScorer(\n    weights={\n        \"confidence\": 0.5,\n        \"relevance\": 0.3,\n        \"novelty\": 0.2,\n    },\n    embedding_provider=embedding_provider\n)\n\n# 2. Novelty filter: Remove near-duplicates\nnovelty_filter = MMRNoveltyFilter(\n    lambda_param=0.6,  # 60% relevance, 40% diversity\n    embedding_provider=embedding_provider\n)\n\n# 3. Budget: Control costs\nbudget = Budget(\n    max_tokens=10000,    # Stop at 10K tokens\n    max_nodes=20,        # Max 20 branches explored\n    max_time_seconds=60  # Max 60 seconds\n)\nbudget_manager = BudgetManager(budget=budget)\n\n# 4. Beam: Top-3 selection\nbeam = BeamSelector(\n    scorer=scorer,\n    k=3\n)\n\n# 5. Main selector (novelty_filter goes HERE, not in BeamSelector!)\nselector = BranchSelector(\n    beam_selector=beam,\n    novelty_filter=novelty_filter,\n    budget_manager=budget_manager\n)\n\n# Test scenario: Question answering\nparent = Candidate(\n    id=\"question\",\n    text=\"What are the benefits of exercise?\",\n    meta={\"logprobs\": -0.1}\n)\n\ncandidates = [\n    Candidate(id=\"c1\", text=\"Exercise improves cardiovascular health and reduces disease risk\", meta={\"logprobs\": -0.2}),\n    Candidate(id=\"c2\", text=\"Regular physical activity boosts heart health and lowers illness\", meta={\"logprobs\": -0.3}),  # Similar to c1\n    Candidate(id=\"c3\", text=\"Working out enhances mental well-being and reduces stress\", meta={\"logprobs\": -0.25}),\n    Candidate(id=\"c4\", text=\"Physical fitness increases energy levels and improves sleep quality\", meta={\"logprobs\": -0.35}),\n    Candidate(id=\"c5\", text=\"Exercise helps with weight management and muscle building\", meta={\"logprobs\": -0.4}),\n    Candidate(id=\"c6\", text=\"Regular workouts strengthen bones and improve flexibility\", meta={\"logprobs\": -0.45}),\n]\n\n# Run the pipeline\nresult = selector.step(parent, candidates)\n\n# Display results\nprint()\nprint(f\"Question: {parent.text}\")\nprint()\nprint(\"Pipeline Configuration:\")\nprint(\"  - Beam Size: Top-3 selection\")\nprint(\"  - Scoring: 50% confidence, 30% relevance, 20% novelty\")\nprint(\"  - Novelty: MMR with 60% relevance balance\")\nprint(\"  - Budget: 10K tokens, 20 nodes, 60 seconds\")\n\nprint()\nprint(f\"[OK] SELECTED RESPONSES ({len(result.kept)}/{len(candidates)}):\")\nfor i, candidate in enumerate(result.kept, 1):\n    print()\n    print(f\"{i}. [{candidate.id}]\")\n    print(f\"   {candidate.text}\")\n    if hasattr(candidate, 'score'):\n        print(f\"   Score: {candidate.score:.3f}\")\n\n# Calculate filtered\nkept_ids = {c.id for c in result.kept}\nfiltered = [c for c in result.scored if c.id not in kept_ids]\n\nprint()\nprint(f\"[X] FILTERED OUT ({len(filtered)}):\")\nfor candidate in filtered:\n    print(f\"   [{candidate.id}] {candidate.text[:50]}...\")\n\n# Budget status\nprint()\nprint(\"Budget Status:\")\nprint(f\"  - Tokens used: {budget_manager.tokens_used} / {budget.max_tokens}\")\nprint(f\"  - Nodes explored: {budget_manager.nodes_explored} / {budget.max_nodes}\")\nbudget_ok = 'Yes' if not budget_manager.is_exhausted() else 'No'\nprint(f\"  - Within budget: {budget_ok}\")\n\nprint()\nprint(\"=\"*60)\nprint(\"[OK] Pipeline complete! High-quality, diverse results.\")\nprint(\"=\"*60)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this demo, you learned:\n",
        "\n",
        "### âœ… What We Covered\n",
        "\n",
        "1. **Basic Beam Search** - Select top-K responses\n",
        "2. **Multi-Strategy Scoring** - Combine confidence, relevance, novelty\n",
        "3. **Novelty Filtering** - Remove duplicates and ensure diversity\n",
        "4. **Complete Pipeline** - Production-ready setup with budget control\n",
        "\n",
        "### ðŸŽ¯ Key Components\n",
        "\n",
        "| Component | Purpose |\n",
        "|-----------|----------|\n",
        "| `BranchSelector` | Main orchestrator |\n",
        "| `BeamSelector` | Top-K selection |\n",
        "| `CompositeScorer` | Multi-strategy scoring |\n",
        "| `NoveltyFilter` | Diversity control |\n",
        "| `BudgetManager` | Cost control |\n",
        "\n",
        "### ðŸ“š Next Steps\n",
        "\n",
        "Ready to dive deeper?\n",
        "\n",
        "1. **Advanced Demo**: Check out the [Creative Writing Demo](https://colab.research.google.com/github/chatroutes/chatroutes-autobranch/blob/master/notebooks/creative_writing_colab.ipynb)\n",
        "   - Full end-to-end example with Ollama\n",
        "   - Real LLM integration\n",
        "   - Multi-turn branching\n",
        "\n",
        "2. **Documentation**:\n",
        "   - [GitHub Repository](https://github.com/chatroutes/chatroutes-autobranch)\n",
        "   - [Quick Start Guide](https://github.com/chatroutes/chatroutes-autobranch/blob/master/QUICKSTART.md)\n",
        "   - [Examples](https://github.com/chatroutes/chatroutes-autobranch/tree/master/examples)\n",
        "\n",
        "3. **Use Cases**:\n",
        "   - Tree-of-thought reasoning\n",
        "   - Multi-agent systems\n",
        "   - Creative writing assistants\n",
        "   - Question answering systems\n",
        "   - Code generation with multiple solutions\n",
        "\n",
        "### ðŸ’¡ Pro Tips\n",
        "\n",
        "- Start with simple beam search, add complexity as needed\n",
        "- Use MMR for best balance of quality and diversity\n",
        "- Always set budgets to prevent runaway costs\n",
        "- Experiment with different scoring weights for your use case\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸš€ Install in Your Project\n",
        "\n",
        "```bash\n",
        "pip install chatroutes-autobranch\n",
        "```\n",
        "\n",
        "### ðŸ“§ Get Help\n",
        "\n",
        "- **Issues**: https://github.com/chatroutes/chatroutes-autobranch/issues\n",
        "- **Discussions**: https://github.com/chatroutes/chatroutes-autobranch/discussions\n",
        "- **Email**: hello@chatroutes.com\n",
        "\n",
        "---\n",
        "\n",
        "**Happy branching!** ðŸŒ³"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ChatRoutes AutoBranch - Getting Started Demo",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}